<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>学习笔记 on 平方君的后花园</title><link>https://i-square.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link><description>Recent content in 学习笔记 on 平方君的后花园</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 04 Feb 2024 16:36:57 +0800</lastBuildDate><atom:link href="https://i-square.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>书生·浦语大模型实战营（六）：OpenCompass大模型评测</title><link>https://i-square.github.io/p/InternLM-tutorial-campsection6-OpenCompass-and-LLM-Evaluation/</link><pubDate>Sun, 04 Feb 2024 16:36:57 +0800</pubDate><guid>https://i-square.github.io/p/InternLM-tutorial-campsection6-OpenCompass-and-LLM-Evaluation/</guid><description>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection6-OpenCompass-and-LLM-Evaluation/OpenCampass.svg" alt="Featured image of post 书生·浦语大模型实战营（六）：OpenCompass大模型评测" />&lt;h2 id="前言">前言&lt;/h2>
&lt;p>本文为&lt;a class="link" href="https://github.com/InternLM/tutorial" target="_blank" rel="noopener"
>书生·浦语大模型实战营&lt;/a>的课程笔记系列第六节&lt;/p>
&lt;ul>
&lt;li>教学视频：&lt;a class="link" href="https://www.bilibili.com/video/BV1Gg4y1U7uc/" target="_blank" rel="noopener"
>B站 BV1Gg4y1U7uc&lt;/a>&lt;/li>
&lt;li>配套文档：&lt;a class="link" href="https://github.com/InternLM/tutorial/blob/main/opencompass/opencompass_tutorial.md" target="_blank" rel="noopener"
>InternLM/tutorial opencompass&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="关于评测的三个问题">关于评测的三个问题&lt;/h2>
&lt;h3 id="为什么需要评测">为什么需要评测&lt;/h3>
&lt;ul>
&lt;li>模型选型&lt;/li>
&lt;li>能力提升&lt;/li>
&lt;li>应用场景效果评测&lt;/li>
&lt;/ul>
&lt;p>对于不同主体的看待角度来说：&lt;/p>
&lt;ul>
&lt;li>普通用户：了解模型的特色能力和实际效果&lt;/li>
&lt;li>开发者：监控模型能力变化，指导优化模型生产&lt;/li>
&lt;li>管理机构：减少大模型带来的社会风险&lt;/li>
&lt;li>产业界：找出最适合产业应用的模型，赋能真实场景&lt;/li>
&lt;/ul>
&lt;h3 id="需要评测什么">需要评测什么&lt;/h3>
&lt;ul>
&lt;li>知识、推理、语言&lt;/li>
&lt;li>长文本、智能体、多轮对话&lt;/li>
&lt;li>情感、认知、价值观。&lt;/li>
&lt;/ul>
&lt;h3 id="怎么样测试大语言模型">怎么样测试大语言模型&lt;/h3>
&lt;ul>
&lt;li>自动化客观评测&lt;/li>
&lt;li>人机交互评测&lt;/li>
&lt;li>基于大模型的大模型评测&lt;/li>
&lt;/ul>
&lt;p>当然，也有其他评测角度，比如利用&lt;strong>提示词工程&lt;/strong>，评测大模型对prompt的敏感性，反映模型鲁棒性&lt;/p>
&lt;h2 id="opencompass">OpenCompass&lt;/h2>
&lt;p>这部分的介绍请参考官方文档和配套教学文档，这里不再赘述&lt;/p>
&lt;ul>
&lt;li>官方仓库： &lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener"
>https://github.com/open-compass/opencompass&lt;/a>&lt;/li>
&lt;li>官方教程： &lt;a class="link" href="https://opencompass.readthedocs.io/zh-cn/latest/" target="_blank" rel="noopener"
>https://opencompass.readthedocs.io/zh-cn/latest/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="作业">作业&lt;/h2>
&lt;h3 id="基础作业">基础作业&lt;/h3>
&lt;blockquote>
&lt;p>使用 OpenCompass 评测 InternLM2-Chat-7B 模型在 C-Eval 数据集上的性能&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>只需要把教学视频和配套文档里的 &lt;code>internlm-chat-7b&lt;/code> 模型修改为 &lt;code>internlm2-chat-7b&lt;/code> 评测即可，模型已经在开发机中了，路径： &lt;code>/share/model_repos/internlm2-chat-7b&lt;/code>&lt;/li>
&lt;li>运行代码：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 在 opencompass 目录和环境下&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">conda&lt;/span> &lt;span class="n">activate&lt;/span> &lt;span class="n">opencompass&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">python&lt;/span> &lt;span class="n">run&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">py&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">datasets&lt;/span> &lt;span class="n">ceval_gen&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">hf&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">path&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">share&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">model_repos&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">internlm2&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">chat&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">7&lt;/span>&lt;span class="n">b&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">tokenizer&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">path&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">share&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">model_repos&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">internlm2&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">chat&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">7&lt;/span>&lt;span class="n">b&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">tokenizer&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">kwargs&lt;/span> &lt;span class="n">padding_side&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;left&amp;#39;&lt;/span> &lt;span class="n">truncation&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;left&amp;#39;&lt;/span> &lt;span class="n">trust_remote_code&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">kwargs&lt;/span> &lt;span class="n">trust_remote_code&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span> &lt;span class="n">device_map&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;auto&amp;#39;&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">seq&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nb">len&lt;/span> &lt;span class="mi">2048&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">out&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nb">len&lt;/span> &lt;span class="mi">16&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">batch&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">size&lt;/span> &lt;span class="mi">4&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gpus&lt;/span> &lt;span class="mi">1&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">debug&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>跑评测耗时还挺久的，跑起来就去干别的了
&lt;ul>
&lt;li>然后由于开发机所剩算力不够被强制停机了，没看到最终结果 :P&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="进阶作业">进阶作业&lt;/h3>
&lt;blockquote>
&lt;p>使用 OpenCompass 评测 InternLM2-Chat-7B 模型使用 LMDeploy 0.2.0 部署后在 C-Eval 数据集上的性能&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>2种思路，时间关系就不实操了
&lt;ol>
&lt;li>使用api评测方式&lt;/li>
&lt;li>使用 LMDeploy 转换的 turbomind 格式，参考教程：&lt;a class="link" href="https://opencompass.readthedocs.io/zh-cn/latest/advanced_guides/evaluation_turbomind.html" target="_blank" rel="noopener"
>评测 LMDEPLOY 模型&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h2 id="后记">后记&lt;/h2>
&lt;p>本节课是实战营的最后一节课了，有始亦有终。&lt;/p>
&lt;p>本次实战营的课程内容涵盖了大模型的全链路开源体系，从基础的模型理解，到趣味的实践Demo，再到知识库的搭建，微调实践，量化部署，以及模型评测，每一节课都在为我打开一个新的视角，让我对大模型有了更全面的认识。&lt;/p>
&lt;p>这是一次丰富的学习经历，期待下一次的学习之旅！&lt;/p></description></item><item><title>书生·浦语大模型实战营（五）：LMDeploy 大模型量化部署实践</title><link>https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/</link><pubDate>Sun, 14 Jan 2024 17:29:20 +0800</pubDate><guid>https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/</guid><description>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/cover.webp" alt="Featured image of post 书生·浦语大模型实战营（五）：LMDeploy 大模型量化部署实践" />&lt;h2 id="前言">前言&lt;/h2>
&lt;p>本文为&lt;a class="link" href="https://github.com/InternLM/tutorial" target="_blank" rel="noopener"
>书生·浦语大模型实战营&lt;/a>的课程笔记系列第五节&lt;/p>
&lt;ul>
&lt;li>教学视频：&lt;a class="link" href="https://www.bilibili.com/video/BV1iW4y1A77P/" target="_blank" rel="noopener"
>B站 BV1iW4y1A77P&lt;/a>&lt;/li>
&lt;li>配套文档：&lt;a class="link" href="https://github.com/InternLM/tutorial/blob/main/lmdeploy/lmdeploy.md" target="_blank" rel="noopener"
>InternLM/tutorial lmdeploy&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="大模型部署背景">大模型部署背景&lt;/h2>
&lt;h3 id="模型部署">模型部署&lt;/h3>
&lt;ul>
&lt;li>定义
&lt;ul>
&lt;li>将训练好的模型在特定软硬件环境中启动的过程，使模型能够接收输入并返回预测结果&lt;/li>
&lt;li>为了满足性能和效率的需求，常常需要对模型进行优化，例如模型压缩和硬件加速&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>产品形态
&lt;ul>
&lt;li>云端、边缘计算端、移动端&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>计算设备
&lt;ul>
&lt;li>CPU、GPU、NPU、TPU 等&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="大模型特点">大模型特点&lt;/h3>
&lt;ul>
&lt;li>内存开销巨大
&lt;ul>
&lt;li>庞大的参数量。7B模型仅权重就需要 14+G 内存&lt;/li>
&lt;li>采用自回归生成 token，需要缓存 Attention 的 k/v，带来巨大的内存开销&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>动态 shape
&lt;ul>
&lt;li>请求数不固定&lt;/li>
&lt;li>Token 逐个生成，且数量不定&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>相对视觉模型，LLM 结构简单
&lt;ul>
&lt;li>Transformers 结构，大部分是 decoder-only&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="大模型部署挑战">大模型部署挑战&lt;/h3>
&lt;ul>
&lt;li>设备
&lt;ul>
&lt;li>如何应对巨大的存储问题？低存储设备（消费级显卡、手机等）如何部署？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>推理
&lt;ul>
&lt;li>如何加速 token 的生成速度&lt;/li>
&lt;li>如何解决动态 shape，让推理可以不间断&lt;/li>
&lt;li>如何有效管理和利用内存&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>服务如何提升系统整体吞吐量？&lt;/li>
&lt;li>对于个体用户，如何降低响应时间？&lt;/li>
&lt;/ul>
&lt;h3 id="大模型部署方案">大模型部署方案&lt;/h3>
&lt;ul>
&lt;li>技术点
&lt;ul>
&lt;li>模型并行&lt;/li>
&lt;li>低比特量化&lt;/li>
&lt;li>Page Attention&lt;/li>
&lt;li>transformer 计算和访存优化&lt;/li>
&lt;li>Continuous Batch&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>方案
&lt;ul>
&lt;li>huggingface transformers&lt;/li>
&lt;li>专门的推理加速框架
&lt;ul>
&lt;li>云端
&lt;ul>
&lt;li>lmdeploy&lt;/li>
&lt;li>vllm&lt;/li>
&lt;li>tensorrt-llm&lt;/li>
&lt;li>deepspeed&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>移动端
&lt;ul>
&lt;li>llama.cpp&lt;/li>
&lt;li>mlc-llm&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="lmdeploy-简介">LMDeploy 简介&lt;/h2>
&lt;p>LMDeploy 是 LLM 在英伟达设备上部署的全流程解决方案。包括模型轻量化、推理和服务。项目地址： &lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener"
>https://github.com/InternLM/lmdeploy&lt;/a>&lt;/p>
&lt;p>LMDeploy 提供以下核心功能（细节详见官方仓库，这里不做赘述）：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>高效推理引擎 TurboMind&lt;/strong>：开发了 Persistent Batch(即 Continuous Batch)，Blocked K/V Cache，动态拆分和融合，张量并行，高效的计算 kernel等重要特性，保障了 LLMs 推理时的高吞吐和低延时。&lt;/li>
&lt;li>&lt;strong>有状态推理&lt;/strong>：通过缓存多轮对话过程中 attention 的 k/v，记住对话历史，从而避免重复处理历史会话。显著提升长文本多轮对话场景中的效率。&lt;/li>
&lt;li>&lt;strong>量化&lt;/strong>：LMDeploy 支持多种量化方式和高效的量化模型推理。在不同规模的模型上，验证了量化的可靠性。&lt;/li>
&lt;/ul>
&lt;h2 id="动手实践环节安装部署量化">动手实践环节——安装、部署、量化&lt;/h2>
&lt;p>跟着教学配套文档：&lt;a class="link" href="https://github.com/InternLM/tutorial/blob/main/lmdeploy/lmdeploy.md" target="_blank" rel="noopener"
>InternLM/tutorial lmdeploy&lt;/a> 一步一步跟下来即可&lt;/p>
&lt;h2 id="作业">作业&lt;/h2>
&lt;h3 id="基础作业">基础作业&lt;/h3>
&lt;blockquote>
&lt;p>目标：使用 LMDeploy 以本地对话、网页Gradio、API服务中的一种方式部署 InternLM-Chat-7B 模型，生成 300 字的小故事（需截图）&lt;/p>
&lt;/blockquote>
&lt;p>作业很简单，只需要几条命令即可，索性3种方式都来一遍，操作流程：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 离线转换&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lmdeploy convert internlm-chat-7b /root/share/temp/model_repos/internlm-chat-7b
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 1. TurboMind 推理+命令行本地对话&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lmdeploy chat turbomind ./workspace
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 2. TurboMind推理+API服务&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># server端&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lmdeploy serve api_server ./workspace &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --server_name 0.0.0.0 &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --server_port &lt;span class="m">8888&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --instance_num &lt;span class="m">64&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --tp &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># client端&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lmdeploy serve api_client http://localhost:8888
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 3. 网页gradio部署&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 先把端口转发一下&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ssh -CNg -L 8008:127.0.0.1:8008 root@ssh.intern-ai.org.cn -p &lt;span class="m">34664&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 启动server&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lmdeploy serve gradio ./workspace --server_port &lt;span class="m">8008&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 本地访问 127.0.0.1:8008进行对话&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;em>使用的prompt：帮我生成一个300字的小故事，主角是打工人叫平方君，内容是他通过不断努力升职加薪、当上总经理、出任CEO、迎娶白富美、走上人生巅峰的励志故事&lt;/em>&lt;/p>
&lt;ul>
&lt;li>本地对话结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/local_chat.png"
width="1826"
height="692"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/local_chat_hu1b070dc5ae132c510d996990e7d32982_94568_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/local_chat_hu1b070dc5ae132c510d996990e7d32982_94568_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="local_chat"
class="gallery-image"
data-flex-grow="263"
data-flex-basis="633px"
>&lt;/p>
&lt;ul>
&lt;li>API服务结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/api_chat.png"
width="1690"
height="762"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/api_chat_hu7b14bb89d694606f7938d2580a20275d_113193_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/api_chat_hu7b14bb89d694606f7938d2580a20275d_113193_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="api_chat"
class="gallery-image"
data-flex-grow="221"
data-flex-basis="532px"
>&lt;/p>
&lt;ul>
&lt;li>网页Gradio结果：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/gradio_chat.png"
width="1810"
height="805"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/gradio_chat_hud19c95c02e191ef2ce4e1b26b957e5d9_38670_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection5-LLM-Quantization-Deployment-Practice-based-on-LMDeploy/homework/gradio_chat_hud19c95c02e191ef2ce4e1b26b957e5d9_38670_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="gradio_chat"
class="gallery-image"
data-flex-grow="224"
data-flex-basis="539px"
>&lt;/p>
&lt;h3 id="进阶作业可选做">进阶作业（可选做）&lt;/h3>
&lt;blockquote>
&lt;p>目标：&lt;/p>
&lt;ul>
&lt;li>将第四节课训练自我认知小助手模型使用 LMDeploy 量化部署到 OpenXLab 平台。&lt;/li>
&lt;li>对internlm-chat-7b模型进行量化，并同时使用KV Cache量化，使用量化后的模型完成API服务的部署，分别对比模型量化前后和 KV Cache 量化前后的显存大小（将 bs设置为 1 和 max len 设置为512）。&lt;/li>
&lt;li>在自己的任务数据集上任取若干条进行Benchmark测试，测试方向包括：&lt;br>
（1）TurboMind推理+Python代码集成&lt;br>
（2）在（1）的基础上采用W4A16量化&lt;br>
（3）在（1）的基础上开启KV Cache量化&lt;br>
（4）在（2）的基础上开启KV Cache量化&lt;br>
（5）使用Huggingface推理&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>由于时间关系，进阶作业没有计划做&lt;/p></description></item><item><title>书生·浦语大模型实战营（四）：XTuner 大模型单卡低成本微调实战</title><link>https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/</link><pubDate>Fri, 12 Jan 2024 09:50:54 +0800</pubDate><guid>https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/</guid><description>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/head.webp" alt="Featured image of post 书生·浦语大模型实战营（四）：XTuner 大模型单卡低成本微调实战" />&lt;h2 id="前言">前言&lt;/h2>
&lt;p>本文为&lt;a class="link" href="https://github.com/InternLM/tutorial" target="_blank" rel="noopener"
>书生·浦语大模型实战营&lt;/a>的课程笔记系列第四节&lt;/p>
&lt;ul>
&lt;li>教学视频：&lt;a class="link" href="https://www.bilibili.com/video/BV1yK4y1B75J/" target="_blank" rel="noopener"
>B站 BV1yK4y1B75J&lt;/a>&lt;/li>
&lt;li>配套文档：&lt;a class="link" href="https://github.com/InternLM/tutorial/blob/main/xtuner/README.md" target="_blank" rel="noopener"
>InternLM/tutorial xtuner&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="finetune简介">Finetune简介&lt;/h2>
&lt;p>LLM 的下游应用中，&lt;strong>增量预训练&lt;/strong>和&lt;strong>指令跟随&lt;/strong>是经常会用到的两种微调模式&lt;/p>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/finetune.webp"
width="1441"
height="105"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/finetune_huef66cf2caa2bdae3207a736027637305_15282_480x0_resize_q75_h2_box_2.webp 480w, https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/finetune_huef66cf2caa2bdae3207a736027637305_15282_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
alt="finetune"
class="gallery-image"
data-flex-grow="1372"
data-flex-basis="3293px"
>&lt;/p>
&lt;h3 id="增量预训练微调">增量预训练微调&lt;/h3>
&lt;ul>
&lt;li>使用场景：让基座模型学习到一些新知识，如某个垂类领域的常识&lt;/li>
&lt;li>训练数据：文章、书籍、代码等&lt;/li>
&lt;li>数据构成：没有 &lt;code>System&lt;/code> 和 &lt;code>Input&lt;/code>，只有 &lt;code>Output&lt;/code>，全部参与loss计算&lt;/li>
&lt;/ul>
&lt;h3 id="指令跟随微调">指令跟随微调&lt;/h3>
&lt;ul>
&lt;li>使用场景：让模型学会对话模板，根据人类指令进行对话&lt;/li>
&lt;li>训练数据：高质量的对话、问答数据&lt;/li>
&lt;li>数据构成：包含 &lt;code>System&lt;/code>、&lt;code>Input&lt;/code> 和 &lt;code>Output&lt;/code> ，但只有 &lt;code>Output&lt;/code> 部分参与loss计算&lt;/li>
&lt;/ul>
&lt;h3 id="对话模板">对话模板&lt;/h3>
&lt;ul>
&lt;li>对话模板是为了能够让 LLM 区分出 &lt;strong>System&lt;/strong>、&lt;strong>User&lt;/strong> 和 &lt;strong>Assistant&lt;/strong>&lt;/li>
&lt;li>不同的模型会有不同的模板&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/chat_template.webp"
width="1693"
height="718"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/chat_template_hu3ec7abb32f2ce92791a8f7db528daec1_73986_480x0_resize_q75_h2_box_2.webp 480w, https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/chat_template_hu3ec7abb32f2ce92791a8f7db528daec1_73986_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
alt="chat_template"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="565px"
>&lt;/p>
&lt;h3 id="lora--qlora">LoRA &amp;amp; QLoRA&lt;/h3>
&lt;h4 id="lora-low-rank-adaptation-of-large-language-models">LoRA: Low-Rank Adaptation of Large Language Models&lt;/h4>
&lt;ul>
&lt;li>LLM 的参数量主要集中在模型中的 Linear, 训练这些参数会耗费大量的显存&lt;/li>
&lt;li>LoRA 通过在原本的 Linear 旁，新增一个支路，包含两个连续的小 linear，新增的这个支路通常叫做 Adapter&lt;/li>
&lt;li>Adapter 参数量远小于原本的 Linear，能大幅降低训练的显存消耗&lt;/li>
&lt;/ul>
&lt;h4 id="qlora-quantized-llms-with-low-rank-adapters">QLoRA: Quantized LLMs with Low-Rank Adapters&lt;/h4>
&lt;ul>
&lt;li>4位NormalFloat量化：这是一种改进量化的方法，确保每个量化仓中有相同数量的值，这避免了计算问题和异常值的错误。&lt;/li>
&lt;li>双量化：对量化常量再次量化以节省额外内存的过程。&lt;/li>
&lt;li>统一内存分页：它依赖于NVIDIA统一内存管理，自动处理CPU和GPU之间的页到页传输，它可以保证GPU处理无错，特别是在GPU可能耗尽内存的情况下。&lt;/li>
&lt;/ul>
&lt;h4 id="全量微调loraqlora对比">全量微调、LoRA、QLoRA对比&lt;/h4>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/finetune_compare.webp"
width="1463"
height="800"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/finetune_compare_hu683ce0fd2081c91d802ac80b2ad43a34_60268_480x0_resize_q75_h2_box_2.webp 480w, https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/finetune_compare_hu683ce0fd2081c91d802ac80b2ad43a34_60268_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
alt="finetune_compare"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="438px"
>&lt;/p>
&lt;h2 id="xtuner简介">XTuner简介&lt;/h2>
&lt;p>详见 &lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener"
>XTuner&lt;/a> 的官方仓库&lt;/p>
&lt;h2 id="xtuner快速上手">XTuner快速上手&lt;/h2>
&lt;p>参考配套教学文档：&lt;a class="link" href="https://github.com/InternLM/tutorial/blob/main/xtuner/README.md" target="_blank" rel="noopener"
>InternLM/tutorial xtuner&lt;/a>&lt;/p>
&lt;h3 id="自定义微调数据">自定义微调数据&lt;/h3>
&lt;p>按照教学文档，实操一遍即可，XTuner 上手确实很简单&lt;/p>
&lt;h3 id="ms-agent-数据集">MS-Agent 数据集&lt;/h3>
&lt;p>这个数据集比较有意思，能够赋予大模型调用api的agent能力，原理：&lt;/p>
&lt;ul>
&lt;li>模型的回复中会包括插件调用代码和执行代码
&lt;ul>
&lt;li>调用代码是 LLM 生成的&lt;/li>
&lt;li>执行代码是需要调用服务来生成结果的，这里我们需要给 &lt;code>xtuner chat&lt;/code> 增加 &lt;code>--lagent&lt;/code> 参数来实现&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="lagent调用实战">lagent调用实战&lt;/h3>
&lt;p>本次继续沿用之前课程配置的 &lt;a class="link" href="https://studio.intern-ai.org.cn/" target="_blank" rel="noopener"
>InternStudio&lt;/a> 平台开发机&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># conda环境创建&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda create --name xtuner0.1.9 &lt;span class="nv">python&lt;/span>&lt;span class="o">=&lt;/span>3.10 -y
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda activate xtuner0.1.9
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> ~ &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> mkdir xtuner019 &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="nb">cd&lt;/span> xtuner019
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">git clone -b v0.1.9 https://gitee.com/Internlm/xtuner
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> xtuner
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install -e &lt;span class="s1">&amp;#39;.[all]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 资源准备&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">mkdir ~/ft-msagent &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="nb">cd&lt;/span> ~/ft-msagent
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cp -r /root/share/temp/model_repos/internlm-chat-7b/ .
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 用modelscope下载微调参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install modelscope
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 保存 download.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="s">&amp;lt;&amp;lt; EOF &amp;gt; download.py
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">from modelscope import snapshot_download
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">model_dir = snapshot_download(&amp;#39;xtuner/internlm-7b-qlora-msagent-react&amp;#39;)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 指定保存路径到当前目录&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">MODELSCOPE_CACHE&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="sb">`&lt;/span>&lt;span class="nb">pwd&lt;/span>&lt;span class="sb">`&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python download.py &lt;span class="c1"># 模型下载到了 xtuner/internlm-7b-qlora-msagent-react&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 设置 serper 环境变量&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">SERPER_API_KEY&lt;/span>&lt;span class="o">=&lt;/span>xxx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 启动&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">xtuner chat ./internlm-chat-7b --adapter xtuner/internlm-7b-qlora-msagent-react --lagent
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 报错的话，按照教学文档处理&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vim /root/xtuner019/xtuner/xtuner/tools/chat.py &lt;span class="c1"># 按文档修改，注释掉139行 &amp;#39;trust_remote_code&amp;#39;: True&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 重新启动&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">xtuner chat ./internlm-chat-7b --adapter xtuner/internlm-7b-qlora-msagent-react --lagent
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="测试验证">测试验证&lt;/h4>
&lt;p>&lt;em>prompt: 你好，西安明天天气怎么样？&lt;/em>&lt;/p>
&lt;ul>
&lt;li>结果图：
&lt;ul>
&lt;li>教学视频里回答失败了，但我自己部署是成功的&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/chat_lagent.png"
width="1248"
height="204"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/chat_lagent_hu5bafbf7b8738257202ae5dda08b0a37c_16529_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/screenshots/chat_lagent_hu5bafbf7b8738257202ae5dda08b0a37c_16529_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="chat_lagent"
class="gallery-image"
data-flex-grow="611"
data-flex-basis="1468px"
>&lt;/p>
&lt;h2 id="作业">作业&lt;/h2>
&lt;h3 id="基础作业">基础作业&lt;/h3>
&lt;blockquote>
&lt;p>目标：构建数据集，使用 XTuner 微调 InternLM-Chat-7B 模型, 让模型学习到它是你的智能小助手，效果如下图所示，本作业训练出来的模型的输出需要&lt;strong>将不要葱姜蒜大佬&lt;/strong>替换成自己名字或昵称！&lt;/p>
&lt;/blockquote>
&lt;p>作业参考文档： &lt;a class="link" href="https://github.com/InternLM/tutorial/blob/main/xtuner/self.md" target="_blank" rel="noopener"
>XTuner InternLM-Chat 个人小助手认知微调实践&lt;/a>&lt;/p>
&lt;ul>
&lt;li>训练最后的 eval chat结果已经有变化了：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/homework/eval_chat.png"
width="945"
height="430"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/homework/eval_chat_hu5e1a3206595256d802f2e2e466aed5e3_32738_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/homework/eval_chat_hu5e1a3206595256d802f2e2e466aed5e3_32738_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="eval_chat"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="527px"
>&lt;/p>
&lt;ul>
&lt;li>Web Demo 结果图：
&lt;ul>
&lt;li>其中前3条是我给的训练数据，后面两条是模型自己学习到的&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/homework/self.png"
width="751"
height="852"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/homework/self_hu86f2eef6c30de5d908adb3ca0ab1f673_21396_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection4-XTuner-based-LLM-Single-GPU-Low-cost-Finetune-Practice/homework/self_hu86f2eef6c30de5d908adb3ca0ab1f673_21396_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="self"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="211px"
>&lt;/p>
&lt;h3 id="进阶作业">进阶作业&lt;/h3>
&lt;blockquote>
&lt;p>目标：&lt;/p>
&lt;ul>
&lt;li>将训练好的Adapter模型权重上传到 OpenXLab、Hugging Face 或者 MoelScope 任一一平台。&lt;/li>
&lt;li>将训练好后的模型应用部署到 OpenXLab 平台，参考部署文档请访问：https://aicarrier.feishu.cn/docx/MQH6dygcKolG37x0ekcc4oZhnCe&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>由于时间关系，进阶作业没有计划做&lt;/p></description></item><item><title>书生·浦语大模型实战营（三）：基于 InternLM 和 LangChain 搭建你的知识库</title><link>https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/</link><pubDate>Thu, 11 Jan 2024 16:56:30 +0800</pubDate><guid>https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/</guid><description>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/head.webp" alt="Featured image of post 书生·浦语大模型实战营（三）：基于 InternLM 和 LangChain 搭建你的知识库" />&lt;h2 id="前言">前言&lt;/h2>
&lt;p>本文为&lt;a class="link" href="https://github.com/InternLM/tutorial" target="_blank" rel="noopener"
>书生·浦语大模型实战营&lt;/a>的课程笔记系列第三节&lt;/p>
&lt;ul>
&lt;li>教学视频：&lt;a class="link" href="https://www.bilibili.com/video/BV1sT4y1p71V/" target="_blank" rel="noopener"
>B站 BV1sT4y1p71V&lt;/a>&lt;/li>
&lt;li>配套文档：&lt;a class="link" href="https://github.com/InternLM/tutorial/blob/main/langchain/readme.md" target="_blank" rel="noopener"
>InternLM/tutorial langchain&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="llm的局限性">LLM的局限性&lt;/h2>
&lt;ul>
&lt;li>知识时效性受限：如何让LLM能够获取最新的知识&lt;/li>
&lt;li>专业能力有限：如何打造垂域大模型&lt;/li>
&lt;li>定制化成本高：如何打造个人专属的LLM应用&lt;/li>
&lt;/ul>
&lt;h3 id="大模型开发范式">大模型开发范式&lt;/h3>
&lt;p>解决局限性问题的两种方式是：&lt;/p>
&lt;h4 id="1-rag">1. RAG&lt;/h4>
&lt;p>RAG: Retrieval Augmented Generation，使用外挂数据库，检索相关知识增强生成结果，特点：&lt;/p>
&lt;ul>
&lt;li>低成本&lt;/li>
&lt;li>可实时更新&lt;/li>
&lt;li>受基座模型影响大&lt;/li>
&lt;li>单次回答知识有限&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/RAG_flow.webp"
width="690"
height="668"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/RAG_flow_hue435df51197f5a26305e3a4eed1f11d8_13242_480x0_resize_q75_h2_box_2.webp 480w, https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/RAG_flow_hue435df51197f5a26305e3a4eed1f11d8_13242_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
alt="RAG_frame"
class="gallery-image"
data-flex-grow="103"
data-flex-basis="247px"
>&lt;/p>
&lt;ul>
&lt;li>流程
&lt;ol>
&lt;li>输入文本转化为向量&lt;/li>
&lt;li>在向量数据库中匹配相似文本&lt;/li>
&lt;li>作为prompt在大模型中寻找答案&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h4 id="2-finetune">2. Finetune&lt;/h4>
&lt;p>也即通称的微调，特点：&lt;/p>
&lt;ul>
&lt;li>可个性化微调&lt;/li>
&lt;li>知识覆盖面广&lt;/li>
&lt;li>成本高昂&lt;/li>
&lt;li>无法实时更新&lt;/li>
&lt;/ul>
&lt;h2 id="基于-langchain-搭建-rag-应用">基于 LangChain 搭建 RAG 应用&lt;/h2>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/rag_based_langchain.webp"
width="1104"
height="790"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/rag_based_langchain_hudd7e07a76c7f5daf5a1b69019809a62c_30560_480x0_resize_q75_h2_box_2.webp 480w, https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/rag_based_langchain_hudd7e07a76c7f5daf5a1b69019809a62c_30560_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
alt="rag_based_langchain"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="335px"
>&lt;/p>
&lt;h3 id="langchain框架简介">LangChain框架简介&lt;/h3>
&lt;p>LangChain 框架是一个开源工具，通过为各种 LLM 提供通用接口来简化应用程序的开发流程，帮助开发者自由构建 LLM 应用。&lt;/p>
&lt;p>LangChain 的核心组成模块：&lt;/p>
&lt;ul>
&lt;li>链 (Chains) ：将组件组合实现端到端应用，通过一个对象封装实现一系列LLM操作&lt;/li>
&lt;li>Eg. 检索问答链，覆盖实现了 RAG （检索增强生成）的全部流程&lt;/li>
&lt;/ul>
&lt;h3 id="构建向量数据库">构建向量数据库&lt;/h3>
&lt;h4 id="加载源文件">加载源文件&lt;/h4>
&lt;ul>
&lt;li>确定源文件类型，针对不同类型的源文件选用不同的加载器
&lt;ul>
&lt;li>核心在于将带格式的文本转化为无格式的字符串&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="文档分块">文档分块&lt;/h4>
&lt;ul>
&lt;li>由于单个文档往往超过模型上下文上限，我们需要对加载的文档进行切分
&lt;ul>
&lt;li>一般按字符串长度进行分割&lt;/li>
&lt;li>可以手动控制分割块的长度和重叠区间长度&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="文档向量化">文档向量化&lt;/h4>
&lt;ul>
&lt;li>使用向量数据库来支持语义检索，需要将文档向量化存入向量数据库
&lt;ul>
&lt;li>可以使用任意一种 Embedding 模型来进行向量化&lt;/li>
&lt;li>可以使用多种支持语义检索的向量数据库，一般使用轻量级的 Chroma&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="搭建知识库助手">搭建知识库助手&lt;/h3>
&lt;h4 id="将-internlm-接入-langchain">将 InternLM 接入 LangChain&lt;/h4>
&lt;ul>
&lt;li>LangChain 支持自定义 LLM，可以直接接入到框架中&lt;/li>
&lt;li>我们只需将 InternLM 部署在本地，并封装一个自定义 LLM 类，调用本地 InternLM 即可&lt;/li>
&lt;/ul>
&lt;h4 id="构建检索问答链">构建检索问答链&lt;/h4>
&lt;ul>
&lt;li>LangChain 提供了检索问答链模版，可以自动实现知识检索、Prompt 嵌入、LLM 问答的全部流程&lt;/li>
&lt;li>将基于 InternLM 的自定义 LLM 和已构建的向量数据库接入到检索问答链的上游&lt;/li>
&lt;li>调用检索问答链，即可实现知识库助手的核心功能&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/query_answer_chain.webp"
width="808"
height="425"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/query_answer_chain_hu7a3dcadb9394bffe8a8eafd5905ccdc3_7082_480x0_resize_q75_h2_box_2.webp 480w, https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/screenshots/query_answer_chain_hu7a3dcadb9394bffe8a8eafd5905ccdc3_7082_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
alt="query_answer_chain"
class="gallery-image"
data-flex-grow="190"
data-flex-basis="456px"
>&lt;/p>
&lt;h4 id="rag-方案优化建议">RAG 方案优化建议&lt;/h4>
&lt;ul>
&lt;li>基于RAG的问答系统性能核心受限于：
&lt;ul>
&lt;li>检索精度&lt;/li>
&lt;li>Prompt性能&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>一些可能的优化点：
&lt;ul>
&lt;li>检索方面：
&lt;ul>
&lt;li>基于语义分割，保证每一个chunk的语义完整&lt;/li>
&lt;li>给每一个chunk生成概括式索引，检索时匹配索引&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Prompt方面
&lt;ul>
&lt;li>迭代优化Prompt策略&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="作业">作业&lt;/h2>
&lt;h3 id="基础作业">基础作业&lt;/h3>
&lt;blockquote>
&lt;p>目标：复现课程知识库助手搭建过程 (截图)&lt;/p>
&lt;/blockquote>
&lt;h4 id="环境配置">环境配置&lt;/h4>
&lt;p>本次沿用上节课程配置的 &lt;a class="link" href="https://studio.intern-ai.org.cn/" target="_blank" rel="noopener"
>InternStudio&lt;/a> 平台开发机，省去了一些环境准备的时间，过程不再赘述，教学文档中有详细步骤。&lt;/p>
&lt;h4 id="构建向量数据库-1">构建向量数据库&lt;/h4>
&lt;ul>
&lt;li>终端命令：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/create_db_cmd.png"
width="2173"
height="278"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/create_db_cmd_hud8229e4d622c983353773f9945aaccd3_24332_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/create_db_cmd_hud8229e4d622c983353773f9945aaccd3_24332_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="create_db_cmd"
class="gallery-image"
data-flex-grow="781"
data-flex-basis="1875px"
>&lt;/p>
&lt;h4 id="web-demo">Web Demo&lt;/h4>
&lt;ul>
&lt;li>终端命令：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/gradio_demo_cmd.png"
width="2180"
height="258"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/gradio_demo_cmd_hua4068ac19bb600449ace1e52d26a44e6_22079_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/gradio_demo_cmd_hua4068ac19bb600449ace1e52d26a44e6_22079_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="gradio_demo_cmd"
class="gallery-image"
data-flex-grow="844"
data-flex-basis="2027px"
>&lt;/p>
&lt;ul>
&lt;li>结果图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/gradio_demo.png"
width="1543"
height="874"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/gradio_demo_hu1cb02b189be1795ff0b6408c8207f27a_31308_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection3-Build-Knowledge-Base-Using-InternLM-and-LangChain/homework/gradio_demo_hu1cb02b189be1795ff0b6408c8207f27a_31308_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="gradio_demo"
class="gallery-image"
data-flex-grow="176"
data-flex-basis="423px"
>&lt;/p>
&lt;ul>
&lt;li>分析：从结果来看，大模型回答出了有关23年12月的问题，这不在它本身训练数据中，证明检索问答链是有效的。&lt;/li>
&lt;/ul>
&lt;h3 id="进阶作业">进阶作业&lt;/h3>
&lt;blockquote>
&lt;p>目标：选择一个垂直领域，收集该领域的专业资料构建专业知识库，并搭建专业问答助手，并在 &lt;a class="link" href="https://openxlab.org.cn/apps" target="_blank" rel="noopener"
>OpenXLab&lt;/a> 上成功部署（截图，并提供应用地址）&lt;/p>
&lt;/blockquote>
&lt;p>由于时间关系，进阶作业没有计划做&lt;/p></description></item><item><title>书生·浦语大模型实战营（二）：轻松玩转书生·浦语大模型趣味Demo</title><link>https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/</link><pubDate>Wed, 10 Jan 2024 15:44:22 +0800</pubDate><guid>https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/</guid><description>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/head.webp" alt="Featured image of post 书生·浦语大模型实战营（二）：轻松玩转书生·浦语大模型趣味Demo" />&lt;h2 id="前言">前言&lt;/h2>
&lt;p>本文为&lt;a class="link" href="https://github.com/InternLM/tutorial" target="_blank" rel="noopener"
>书生·浦语大模型实战营&lt;/a>的课程笔记系列第二节&lt;/p>
&lt;ul>
&lt;li>教学视频：&lt;a class="link" href="https://www.bilibili.com/video/BV1Ci4y1z72H/" target="_blank" rel="noopener"
>B站 BV1Ci4y1z72H&lt;/a>&lt;/li>
&lt;li>配套文档：&lt;a class="link" href="https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md" target="_blank" rel="noopener"
>InternLM/tutorial helloworld&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="笔记">笔记&lt;/h2>
&lt;p>由于配套的说明文档已经记录的相当详细并且图文并茂，这节课丝毫没有记录笔记的必要，所以本文仅仅记录课后作业&lt;/p>
&lt;h2 id="基础作业">基础作业&lt;/h2>
&lt;h3 id="hf模型下载">HF模型下载&lt;/h3>
&lt;blockquote>
&lt;p>目标：熟悉 &lt;code>hugging face&lt;/code> 下载功能，使用 &lt;code>huggingface_hub&lt;/code> python 包，下载 &lt;code>InternLM-20B&lt;/code> 的 &lt;code>config.json&lt;/code> 文件到本地（需截图下载过程）。&lt;/p>
&lt;/blockquote>
&lt;p>这个无需多言，只是基本的命令使用，需要注意的是，由于众所周知的原因，国内直接下载 &lt;code>hugging face&lt;/code> 是不行的，这里使用镜像站 &lt;a class="link" href="https://hf-mirror.com/" target="_blank" rel="noopener"
>hf-mirror&lt;/a>。&lt;/p>
&lt;ul>
&lt;li>结果图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/0_hf_download.png"
width="1621"
height="776"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/0_hf_download_hu499ec520d3afab854757ac9da67bb810_11368_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/0_hf_download_hu499ec520d3afab854757ac9da67bb810_11368_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="0_hf_download"
class="gallery-image"
data-flex-grow="208"
data-flex-basis="501px"
>&lt;/p>
&lt;h3 id="浦语7b模型部署demo">浦语7B模型部署demo&lt;/h3>
&lt;blockquote>
&lt;p>目标：使用 InternLM-Chat-7B 模型生成 300 字的小故事（需截图）。&lt;/p>
&lt;/blockquote>
&lt;p>这里我们可以使用两种demo来完成，分别是 &lt;code>cli_demo&lt;/code> 和 &lt;code>web_demo&lt;/code>，详细步骤参考配套教学文档，以下为大致总结：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>环境准备：&lt;/strong>
&lt;ul>
&lt;li>在 &lt;a class="link" href="https://studio.intern-ai.org.cn/" target="_blank" rel="noopener"
>InternStudio&lt;/a> 平台选择 A100(1/4) 的配置，使用 &lt;code>Cuda11.7-conda&lt;/code> 镜像。&lt;/li>
&lt;li>打开开发机，进入终端，切换到 &lt;code>bash&lt;/code> 环境。&lt;/li>
&lt;li>使用提供的脚本克隆并激活 &lt;code>pytorch 2.0.1&lt;/code> 的 &lt;code>conda&lt;/code> 环境，然后安装所需依赖。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>模型下载：&lt;/strong>
&lt;ul>
&lt;li>复制已准备好的 &lt;code>InternLM&lt;/code> 模型到指定目录，或使用 &lt;code>modelscope&lt;/code> 中的 &lt;code>snapshot_download&lt;/code> 函数下载模型（&lt;strong>推荐&lt;/strong>，跑满带宽），或在 &lt;code>huggingface&lt;/code>、&lt;code>OpenXLab&lt;/code> 等处下载。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>代码准备：&lt;/strong>
&lt;ul>
&lt;li>在 &lt;code>/root&lt;/code> 路径下新建 &lt;code>code&lt;/code> 目录，&lt;code>clone&lt;/code> 指定版本的代码。&lt;/li>
&lt;li>在 &lt;code>/root/code/InternLM&lt;/code> 目录下新建 &lt;code>cli_demo.py&lt;/code> 文件，使用 &lt;code>transformers&lt;/code> 和 &lt;code>torch&lt;/code> 库运行大模型。&lt;/li>
&lt;li>修改 &lt;code>/root/code/InternLM/web_demo.py&lt;/code> 中的模型路径为本地路径。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>终端运行：&lt;/strong>
&lt;ul>
&lt;li>运行 &lt;code>xxx_demo.py&lt;/code> 文件，即可体验 &lt;code>InternLM-Chat-7B&lt;/code> 模型的对话能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>&lt;em>使用的prompt：帮我生成一个300字的小故事，主角是打工人叫平方君，内容是他通过不断努力升职加薪、当上总经理、出任CEO、迎娶白富美、走上人生巅峰的励志故事&lt;/em>&lt;/p>
&lt;h4 id="cli_demo">cli_demo&lt;/h4>
&lt;ul>
&lt;li>结果图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_cli_demo.png"
width="1577"
height="274"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_cli_demo_hu99fda8c5f448e7626395faf60b70fb9d_12647_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_cli_demo_hu99fda8c5f448e7626395faf60b70fb9d_12647_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="1_cli_demo"
class="gallery-image"
data-flex-grow="575"
data-flex-basis="1381px"
>&lt;/p>
&lt;h4 id="web_demo">web_demo&lt;/h4>
&lt;p>由于 &lt;a class="link" href="https://studio.intern-ai.org.cn/" target="_blank" rel="noopener"
>InternStudio&lt;/a> 平台的开发机不能直接通过web访问，所以需要做一下端口映射，原理是利用 &lt;code>ssh&lt;/code> 做端口转发。&lt;/p>
&lt;p>在本地主机运行：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssh -CNg -L 8008:127.0.0.1:8008 root@ssh.intern-ai.org.cn -p &lt;span class="m">34664&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>各部分解释如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ssh:&lt;/strong> 启动SSH客户端程序。&lt;/li>
&lt;li>&lt;strong>-C:&lt;/strong> 启用压缩。数据传输时进行压缩，提高传输效率。&lt;/li>
&lt;li>&lt;strong>-N:&lt;/strong> 不执行任何命令，主要用于纯粹建立连接。在这里，它告诉SSH客户端不要执行远程命令。&lt;/li>
&lt;li>&lt;strong>-g:&lt;/strong> 允许远程主机连接到本地的转发端口。在这里，它允许其他主机连接到本地端口8008。&lt;/li>
&lt;li>&lt;strong>-L 8008:127.0.0.1:8008:&lt;/strong> 设置本地端口转发。将本地端口8008转发到远程主机的127.0.0.1（即本地主机）的8008端口。&lt;/li>
&lt;li>&lt;strong>&lt;a class="link" href="mailto:root@ssh.intern-ai.org.cn" >root@ssh.intern-ai.org.cn&lt;/a>:&lt;/strong> 远程SSH服务器的用户名和主机地址。&lt;/li>
&lt;li>&lt;strong>-p 34664:&lt;/strong> 指定SSH服务器的端口号。&lt;/li>
&lt;/ul>
&lt;p>此命令的目的是在本地端口&lt;code>8008&lt;/code>上创建一个SSH隧道，将流量转发到远程服务器上的相同端口，同时允许其他主机通过该远程服务器连接到本地端口。&lt;/p>
&lt;hr>
&lt;p>作业部分：&lt;/p>
&lt;ul>
&lt;li>终端命令：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_web_demo_cmd.png"
width="1199"
height="315"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_web_demo_cmd_hu21a629fe185c1cf89f832c6c759b0d48_11983_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_web_demo_cmd_hu21a629fe185c1cf89f832c6c759b0d48_11983_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="1_web_demo_cmd"
class="gallery-image"
data-flex-grow="380"
data-flex-basis="913px"
>&lt;/p>
&lt;ul>
&lt;li>结果图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_web_demo.png"
width="744"
height="783"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_web_demo_hu710490253f319d20449554139d9d539f_37125_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/1_web_demo_hu710490253f319d20449554139d9d539f_37125_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="1_web_demo"
class="gallery-image"
data-flex-grow="95"
data-flex-basis="228px"
>&lt;/p>
&lt;h2 id="进阶作业可选做">进阶作业（可选做）&lt;/h2>
&lt;h3 id="lagent部署demo">Lagent部署demo&lt;/h3>
&lt;blockquote>
&lt;p>目标：完成 &lt;code>Lagent&lt;/code> 工具调用 Demo 创作部署（需截图）&lt;/p>
&lt;/blockquote>
&lt;p>由于涉及到图形化操作，这里只有 &lt;code>web_demo&lt;/code>，详细步骤参考配套教学文档，以下为大致总结：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>环境准备：&lt;/strong>
&lt;ul>
&lt;li>沿用之前的环境。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>模型下载：&lt;/strong>
&lt;ul>
&lt;li>不再赘述。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Lagent 安装和修改代码：&lt;/strong>
&lt;ul>
&lt;li>切换到 &lt;code>/root/code&lt;/code> 目录，克隆 &lt;code>lagent&lt;/code> 仓库，并通过 &lt;code>pip install -e .&lt;/code> 源码安装。&lt;/li>
&lt;li>修改 &lt;code>react_web_demo.py&lt;/code> 文件，替换为相应代码。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Demo 运行：&lt;/strong>
&lt;ul>
&lt;li>在终端运行 &lt;code>streamlit&lt;/code> 命令，启动 &lt;code>Web&lt;/code> 页面。&lt;/li>
&lt;li>在浏览器中访问 &lt;code>http://127.0.0.1:8008&lt;/code> 查看 Demo。&lt;/li>
&lt;li>选择 &lt;code>InternLM&lt;/code> 模型，输入问题，观察 &lt;code>Lagent&lt;/code> 调度并处理的过程。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>作业部分：&lt;/p>
&lt;ul>
&lt;li>终端命令：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/3_lagent_web_demo_cmd.png"
width="1215"
height="372"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/3_lagent_web_demo_cmd_huf9307425b8e4bd7b19b23abd694151b4_22417_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/3_lagent_web_demo_cmd_huf9307425b8e4bd7b19b23abd694151b4_22417_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="3_lagent_web_demo_cmd"
class="gallery-image"
data-flex-grow="326"
data-flex-basis="783px"
>&lt;/p>
&lt;ul>
&lt;li>结果图：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/3_lagent_web_demo.png"
width="1152"
height="990"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/3_lagent_web_demo_hu8077bf684fa137e1dd412753bccca3a1_25738_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/3_lagent_web_demo_hu8077bf684fa137e1dd412753bccca3a1_25738_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="3_lagent_web_demo"
class="gallery-image"
data-flex-grow="116"
data-flex-basis="279px"
>&lt;/p>
&lt;h3 id="浦语灵笔部署demo">浦语·灵笔部署demo&lt;/h3>
&lt;blockquote>
&lt;p>目标：完成浦语·灵笔的图文理解及创作部署（需截图）&lt;/p>
&lt;/blockquote>
&lt;p>由于涉及到图形化操作，这里也只有 &lt;code>web_demo&lt;/code>，详细步骤参考配套教学文档，以下为大致总结：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>环境准备：&lt;/strong>
&lt;ul>
&lt;li>继续沿用之前的环境。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>模型下载：&lt;/strong>
&lt;ul>
&lt;li>用同样的方式，准备 &lt;code>internlm-xcomposer-7b&lt;/code> 模型到指定目录。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>代码准备：&lt;/strong>
&lt;ul>
&lt;li>在 &lt;code>/root/code&lt;/code> 目录下克隆 &lt;code>InternLM-XComposer&lt;/code> 仓库的代码，切换到指定的 commit 版本，以便对齐教学结果。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Demo 运行：&lt;/strong>
&lt;ul>
&lt;li>在终端运行 &lt;code>web_demo.py&lt;/code> 文件，启动 &lt;code>Web&lt;/code> 页面。&lt;/li>
&lt;li>在浏览器中访问 &lt;code>http://127.0.0.1:8008&lt;/code>，体验图文理解创作的功能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>作业部分： 从终端的log来看，浦语·灵笔的创作流程大致上是先用语言模型生成文章，再选取合适的图片插入点，然后在数据库里根据关键词搜索匹配的图片，之后下载图片，并组合生成一份 &lt;code>markdown&lt;/code> 文档。&lt;/p>
&lt;ul>
&lt;li>终端命令：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo_cmd.png"
width="2206"
height="5648"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo_cmd_huc420ff8248919de7c2b5d7333db1365b_566511_480x0_resize_box_3.png 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo_cmd_huc420ff8248919de7c2b5d7333db1365b_566511_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="4_xcomposer_web_demo_cmd"
class="gallery-image"
data-flex-grow="39"
data-flex-basis="93px"
>&lt;/p>
&lt;ul>
&lt;li>结果图1-图文生成：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo1.webp"
width="1649"
height="1314"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo1_hu40027a68ac8fe4cd5dcffe9b97848ec3_90100_480x0_resize_q75_h2_box_2.webp 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo1_hu40027a68ac8fe4cd5dcffe9b97848ec3_90100_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
alt="4_xcomposer_web_demo1"
class="gallery-image"
data-flex-grow="125"
data-flex-basis="301px"
>&lt;/p>
&lt;ul>
&lt;li>结果图2-多模态对话：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo2.webp"
width="1581"
height="1105"
srcset="https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo2_hu22c55a1ac784658fb8301f51d84583ab_54784_480x0_resize_q75_h2_box_2.webp 480w, https://i-square.github.io/p/InternLM-tutorial-campsection2-Easy-Fun-with-InternLM-Entertaining-Demo/homework/4_xcomposer_web_demo2_hu22c55a1ac784658fb8301f51d84583ab_54784_1024x0_resize_q75_h2_box_2.webp 1024w"
loading="lazy"
alt="4_xcomposer_web_demo2"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="343px"
>&lt;/p></description></item><item><title>书生·浦语大模型实战营（一）：书生·浦语大模型全链路开源体系</title><link>https://i-square.github.io/p/InternLM-tutorial-campsection1-LLM-Full-Stack-Open-Source-Ecosystem/</link><pubDate>Tue, 09 Jan 2024 18:50:55 +0800</pubDate><guid>https://i-square.github.io/p/InternLM-tutorial-campsection1-LLM-Full-Stack-Open-Source-Ecosystem/</guid><description>&lt;img src="https://i-square.github.io/p/InternLM-tutorial-campsection1-LLM-Full-Stack-Open-Source-Ecosystem/camp.webp" alt="Featured image of post 书生·浦语大模型实战营（一）：书生·浦语大模型全链路开源体系" />&lt;h2 id="前言">前言&lt;/h2>
&lt;p>本文为&lt;a class="link" href="https://github.com/InternLM/tutorial" target="_blank" rel="noopener"
>书生·浦语大模型实战营&lt;/a>的课程笔记系列第一节，课程地址：&lt;a class="link" href="https://www.bilibili.com/video/BV1Rc411b7ns/" target="_blank" rel="noopener"
>https://www.bilibili.com/video/BV1Rc411b7ns/&lt;/a>&lt;/p>
&lt;h2 id="从专用模型到通用大模型">从专用模型到通用大模型&lt;/h2>
&lt;p>在过去，人工智能领域的发展一直遵循着一个基本原则：一个模型对应一个场景或者任务。然而，随着技术的进步和需求的增长，这一格局正在发生深刻的变化。如今，我们正迈向一个新的时代，一个模型不再局限于一个场景或任务，而是可以应用于多个场景、多模态的复杂环境中。&lt;/p>
&lt;h2 id="书生浦语大模型发展历程">书生·浦语大模型发展历程&lt;/h2>
&lt;p>书生·浦语大模型的发展历程彰显了这一变革的重要性。它从轻量级的7B社区模型，逐步升级到中量级的20B商业模型，再到重量级的123B全场景模型。这一演进不仅仅是在模型规模上的提升，更是对多模态、多场景应用需求的积极响应。&lt;/p>
&lt;h3 id="interlm-20b全面领先的开源模型">InterLM-20B：全面领先的开源模型&lt;/h3>
&lt;p>&lt;a class="link" href="https://github.com/InternLM/InternLM" target="_blank" rel="noopener"
>InterLM-20B&lt;/a>是一款千亿参数级别的开源模型，其性能在全球范围内处于领先地位。与相近规模的Llama-33B、Llama2-13B以及国内主流的7B、13B开源模型相比，InterLM-20B在不足三分之一的参数量下，却达到了Llama2-70B的水平。&lt;/p>
&lt;h2 id="从模型到应用六个关键步骤">从模型到应用：六个关键步骤&lt;/h2>
&lt;h3 id="第一步模型选型">第一步：模型选型&lt;/h3>
&lt;p>在应用场景中，根据多个大模型的相关维度进行能力比较，并进行模型评测。初步选型后，可确定意向大模型。&lt;/p>
&lt;h3 id="第二步评估业务场景复杂度">第二步：评估业务场景复杂度&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>业务场景简单：&lt;/strong> 如果业务场景不太复杂，可以直接将选定的模型应用于场景中。&lt;/li>
&lt;li>&lt;strong>业务场景复杂：&lt;/strong> 对于复杂场景，通常直接使用开源模型难以满足需求，需要进一步微调、进行prompt工程等构建工作。&lt;/li>
&lt;/ul>
&lt;h3 id="第三步判断微调策略">第三步：判断微调策略&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>全参数微调：&lt;/strong> 若算力足够，可以进行全参数微调，提高模型性能。&lt;/li>
&lt;li>&lt;strong>部分参数微调：&lt;/strong> 如果算力受限，只能进行部分参数微调，固定大部分参数，调整一小部分参数。&lt;/li>
&lt;/ul>
&lt;h3 id="第四步构建智能体">第四步：构建智能体&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>模型与环境交互：&lt;/strong> 考虑模型与环境的交互，特别是如果需要调用外部API或与已有业务数据库交互，则需要构建智能体。&lt;/li>
&lt;li>&lt;strong>无环境交互：&lt;/strong> 如果模型在业务场景中不需要与环境进行交互，可以直接将微调好的模型应用于场景。&lt;/li>
&lt;/ul>
&lt;h3 id="第五步模型评测与应用上线">第五步：模型评测与应用上线&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>模型评测：&lt;/strong> 进行模型评测，确保在实际场景中表现良好。&lt;/li>
&lt;li>&lt;strong>上线或迭代：&lt;/strong> 根据评测结果，决定是否上线应用或者继续迭代模型。&lt;/li>
&lt;/ul>
&lt;h3 id="第六步模型部署">第六步：模型部署&lt;/h3>
&lt;p>考虑软件系统相关性能、安全、功能等方面内容：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>资源优化：&lt;/strong> 考虑如何以更少的资源部署模型。&lt;/li>
&lt;li>&lt;strong>吞吐量提升：&lt;/strong> 提升整个应用的吞吐量，确保在生产环境中的性能表现。&lt;/li>
&lt;/ul>
&lt;p>这六个步骤构成了从选择模型到应用部署的全链条，确保在实际应用中大模型能够充分发挥作用。&lt;/p>
&lt;h2 id="书生-浦语全链条开源开放体系">书生-浦语全链条开源开放体系&lt;/h2>
&lt;p>书生·浦语大模型打破了传统的人工智能应用模式，提出了全链条开源开放体系。这一体系涵盖了从数据到预训练、微调、部署、评测到应用的全过程，为通用人工智能的实现提供了完整的解决方案。数据（&lt;a class="link" href="https://github.com/opendatalab/WanJuan1.0" target="_blank" rel="noopener"
>书生·万卷&lt;/a>）作为起点，经过&lt;a class="link" href="https://github.com/InternLM/InternLM" target="_blank" rel="noopener"
>IntermLM-Train&lt;/a>的预训练，使用&lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener"
>XTuner&lt;/a>进行微调，通过&lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener"
>LMDeploy&lt;/a>实现部署，通过&lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener"
>OpenCompass&lt;/a>进行全面评测，最终应用在&lt;a class="link" href="https://github.com/InternLM/Lagent" target="_blank" rel="noopener"
>Lagent&lt;/a>构建的多模态智能体中。&lt;/p>
&lt;p>这一全链条开源开放体系，为大模型的发展提供了创新性的方法，促使人工智能更好地服务于多样化的现实需求。&lt;/p>
&lt;h3 id="数据覆盖多模态和任务">数据：覆盖多模态和任务&lt;/h3>
&lt;p>全链条开源体系以&lt;a class="link" href="https://github.com/opendatalab/WanJuan1.0" target="_blank" rel="noopener"
>书生-万卷&lt;/a>为基础，涵盖了多模态和多任务的数据需求，为模型的学习提供了全面支持。&lt;/p>
&lt;h4 id="opendatalab开放数据平台">OpenDataLab：开放数据平台&lt;/h4>
&lt;p>&lt;a class="link" href="https://github.com/opendatalab" target="_blank" rel="noopener"
>OpenDataLab&lt;/a>作为开放数据平台，不仅包含丰富多样的开放数据，还为大模型的发展提供了数据支持和实验平台。&lt;/p>
&lt;h3 id="预训练并行训练极致优化">预训练：并行训练，极致优化&lt;/h3>
&lt;p>&lt;a class="link" href="https://github.com/InternLM/InternLM" target="_blank" rel="noopener"
>InterLM&lt;/a>采用并行训练的方式，通过极致优化实现了高效的预训练，为模型的通用性奠定基础。&lt;/p>
&lt;h3 id="微调xtuner支持全参数微调支持lora等低成本微调">微调：XTuner，支持全参数微调，支持Lora等低成本微调&lt;/h3>
&lt;p>微调阶段使用&lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener"
>XTuner&lt;/a>工具，支持全参数微调，同时还支持诸如Lora等低成本微调方法，使模型更好地适应各种特定任务。&lt;/p>
&lt;p>特性：&lt;/p>
&lt;ul>
&lt;li>增量续训：让基座模型学习新知识，垂直领域&lt;/li>
&lt;li>有监督微调：让模型学会理解和遵循各种指令。一般采用全量参数微调和部分参数微调等方法。&lt;/li>
&lt;li>多种微调算法：多种微调策略与算法，覆盖各类SFT场景。&lt;/li>
&lt;li>适配多种开源生态：支持加载HuggingFace、ModelScope模型或者数据级&lt;/li>
&lt;li>自动优化加速：开发者无需关注复杂的显存优化和计算加速细节&lt;/li>
&lt;/ul>
&lt;h3 id="部署lmdeploy全链路部署性能领先">部署：LMDeploy，全链路部署，性能领先&lt;/h3>
&lt;p>&lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener"
>LMDeploy&lt;/a>提供了全链路部署的解决方案，包括模型轻量化、推理和服务，使得大模型在GPU上的部署更加高效，性能领先。&lt;/p>
&lt;h3 id="评测opencompass全方位评测性能可以复现全球领先的大模型开源评测体系">评测：OpenCompass，全方位评测，性能可以复现，全球领先的大模型开源评测体系&lt;/h3>
&lt;p>评测阶段使用&lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener"
>OpenCompass&lt;/a>工具，全方位评测模型性能，保证了评测结果的复现性，成为全球领先的大模型开源评测体系。&lt;/p>
&lt;p>特性：&lt;/p>
&lt;ul>
&lt;li>丰富模型支持：开源模型、API模型一站式评测。&lt;/li>
&lt;li>分布式高效评测：支持千亿参数模型在海量数据集上分布式评测。&lt;/li>
&lt;li>便捷的数据集接口：支持社区用户根据自身需求快速添加自定义数据集。&lt;/li>
&lt;li>敏捷的能力迭代：每周更新大模型能力榜单。&lt;/li>
&lt;/ul>
&lt;h3 id="应用legentagentlego-支持多种智能体支持代码解释器和多种工具">应用：Legent、AgentLego 支持多种智能体，支持代码解释器和多种工具&lt;/h3>
&lt;p>最终，模型的应用在&lt;a class="link" href="https://github.com/InternLM/Lagent" target="_blank" rel="noopener"
>Legent&lt;/a>和&lt;a class="link" href="https://github.com/InternLM/Lagent" target="_blank" rel="noopener"
>AgentLego&lt;/a>等多种智能体中得以体现，支持代码解释器和多种工具，实现了多模态智能体的灵活应用。&lt;/p>
&lt;p>特性：&lt;/p>
&lt;ul>
&lt;li>丰富的工具集合，尤其是提供了大量视觉、多模态相关领域的工具。&lt;/li>
&lt;li>支持多个主流智能体系统，如LangChain、Transformers Agent、Lagent等。&lt;/li>
&lt;li>灵活的多模态工具调用接口，可以轻松支持各类输入输出格式的工具函数&lt;/li>
&lt;li>一键式远程工具部署，轻松使用和调试大模型智能体。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>&lt;strong>相关链接：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>书生·浦语大模型实战营地址：&lt;a class="link" href="https://github.com/InternLM/tutorial" target="_blank" rel="noopener"
>https://github.com/InternLM/tutorial&lt;/a>&lt;/li>
&lt;li>书生·万卷开源地址：&lt;a class="link" href="https://github.com/opendatalab/WanJuan1.0" target="_blank" rel="noopener"
>https://github.com/opendatalab/WanJuan1.0&lt;/a>&lt;/li>
&lt;li>InternLM开源地址：&lt;a class="link" href="https://github.com/InternLM/InternLM" target="_blank" rel="noopener"
>https://github.com/InternLM/InternLM&lt;/a>&lt;/li>
&lt;li>XTuner开源地址：&lt;a class="link" href="https://github.com/InternLM/xtuner" target="_blank" rel="noopener"
>https://github.com/InternLM/xtuner&lt;/a>&lt;/li>
&lt;li>LMDeploy开源地址：&lt;a class="link" href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener"
>https://github.com/InternLM/lmdeploy&lt;/a>&lt;/li>
&lt;li>OpenCompass开源地址：&lt;a class="link" href="https://github.com/open-compass/opencompass" target="_blank" rel="noopener"
>https://github.com/open-compass/opencompass&lt;/a>&lt;/li>
&lt;li>OpenDataLab地址：&lt;a class="link" href="https://opendatalab.org.cn/" target="_blank" rel="noopener"
>https://opendatalab.org.cn/&lt;/a>&lt;/li>
&lt;li>OpenDataLab开源地址：&lt;a class="link" href="https://github.com/opendatalab" target="_blank" rel="noopener"
>https://github.com/opendatalab&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>数据结构学习笔记（八）：不相交集类</title><link>https://i-square.github.io/p/Data-structure-study-notes-8-Disjoint-set-classes/</link><pubDate>Mon, 24 Apr 2017 15:42:58 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-8-Disjoint-set-classes/</guid><description>&lt;p>这一章介绍解决等价问题的一种有效数据结构。实现简单，也非常快，每种操作只需要常数平均时间。&lt;/p>
&lt;h2 id="等价关系-equivalence-relation">等价关系 (equivalence relation)&lt;/h2>
&lt;p>若对于每一对元素(a,b),a,b∈S, &lt;code>a R b&lt;/code>或者为true或者为false，则称在集合S上定义关系R。如果&lt;code>a R b&lt;/code>为true，我们说a和b有关系。&lt;/p>
&lt;p>&lt;strong>等价关系&lt;/strong>是满足下列三个性质的关系R：&lt;/p>
&lt;ol>
&lt;li>自反性：对于所有的a∈S，&lt;code>a R a&lt;/code>&lt;/li>
&lt;li>对称性：&lt;code>a R b&lt;/code>当且仅当&lt;code>b R a&lt;/code>&lt;/li>
&lt;li>传递性：若&lt;code>a R b&lt;/code>且b R c则&lt;code>a R c&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>元素a∈S的&lt;strong>等价类&lt;/strong>(equivalence class)是S的子集，它包含所有与a有（等价）关系的元素。注意，等价类形成对S的一个划分：S的每一个成员恰好出现在一个等价类中。为确定是否a~b，我们只需验证a和b是否都在同一个等价类中。&lt;/p>
&lt;p>输入数据最初是N个集合(collection)的类，每个集合含有一个元素。初始的描述是所有的关系均为false（自反的关系除外）。每个集合都有一个不同的元素，从而&lt;code>Si∩Sj=⊙&lt;/code>，称为&lt;strong>不相交&lt;/strong>(disjoint)&lt;/p>
&lt;p>基本操作有两种，称为&lt;strong>求并/查找&lt;/strong>(union/find)算法。&lt;/p>
&lt;h2 id="灵巧求并算法">灵巧求并算法&lt;/h2>
&lt;p>直观的union操作相当随意，它简单地通过使第二棵树成为第一棵树的子树而完成合并。对其进行简单改进，使得总是较小的树成为较大的树的子树，称为&lt;strong>按大小求并&lt;/strong>(union by size)，它保证树的深度最大是O(logN)。&lt;br>
连续M次操作平均需要O(M)时间。&lt;/p>
&lt;p>另一种方法是&lt;strong>按高度求并&lt;/strong>(union by height)，它同样保证树的深度最大是O(logN)。做法是使浅的树成为深的树的子树。&lt;/p>
&lt;h2 id="一个应用">一个应用&lt;/h2>
&lt;p>应用求并/查找数据结构的一个例子是迷宫的生成。初始化时所有格子都在自己的等价类中，之后不断合并，最终生成迷宫。&lt;/p></description></item><item><title>数据结构学习笔记（七）：排序</title><link>https://i-square.github.io/p/Data-structure-study-notes-7-sorting/</link><pubDate>Sun, 23 Apr 2017 22:01:12 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-7-sorting/</guid><description>&lt;p>在内存里的排序称为内部排序，而在磁盘上的排序称为外部排序。&lt;br>
假设输入数据支持&amp;quot;&amp;lt;&amp;ldquo;和&amp;rdquo;&amp;gt;&amp;ldquo;操作符，除赋值运算外，这种运算是仅有的允许对输入数据进行的操作，在此条件下的排序称为基于比较的排序。&lt;/p>
&lt;h2 id="内容">内容&lt;/h2>
&lt;p>对内部排序的考查将指出：&lt;/p>
&lt;ul>
&lt;li>存在几种直观的算法以O(N^2)排序，如冒泡、选择、插入排序&lt;/li>
&lt;li>希尔排序编程简单，以o(N^2)运行，在实践中很有效&lt;/li>
&lt;li>还有一些稍微复杂的O(NlogN)算法&lt;/li>
&lt;li>任何只使用比较的排序算法在最坏情形下和平均情形下均需要Ω(NlogN)次比较&lt;/li>
&lt;/ul>
&lt;h2 id="插入排序-insertion-sort">插入排序 (insertion sort)&lt;/h2>
&lt;p>插入排序由N-1趟（pass）排序组成，排序策略是，在第p趟，将位置p上的元素向左移动至它在前p+1个元素中的正确位置上。&lt;/p>
&lt;h3 id="分析">分析&lt;/h3>
&lt;p>O(N^2) 精确界，反序输入可达。&lt;br>
若已排序输入，则O(N)&lt;br>
平均情形Θ(N^2)&lt;/p>
&lt;h2 id="一些简单排序算法的下界">一些简单排序算法的下界&lt;/h2>
&lt;p>定理1 N个互异元素的数组的平均逆序数是N(N-1)/4&lt;br>
定理2 通过交换相邻元素进行排序的任何算法平均需要Ω(N^2)时间&lt;br>
对冒泡排序、选择排序、插入排序都有效&lt;br>
定理2告诉我们，为了以o(N^2)排序，必须执行比较，特别是要对相距较远的元素进行交换。排序通过删除逆序得以继续进行，为了有效进行，必须每次交换删除多个逆序。&lt;/p>
&lt;h2 id="希尔排序-shell-sort">希尔排序 (shell sort)&lt;/h2>
&lt;p>发明者是Donald Shell，该算法是冲破二次时间屏障的第一批算法之一，不过，直到它最初被发现的若干年后才证明了它的亚二次时间界。&lt;/p>
&lt;p>它通过比较相距一定间隔的元素来工作，各趟比较所用的距离随着算法的进行而减小，直到只比较相邻元素的最后一趟排序为止。因此，希尔排序又是也叫做&lt;strong>缩减增量排序&lt;/strong>(diminishing increment sort)&lt;/p>
&lt;h3 id="分析-1">分析&lt;/h3>
&lt;p>使用希尔增量的最坏情形Θ(N^2)&lt;br>
Hibbard增量：1，3，7，…… ，2^k - 1&lt;br>
使用Hibbard增量的最坏情形Θ(N^(3/2))&lt;br>
Sedgewick提出了几种增量序列，最坏情形时间O(N^(4/3))&lt;br>
希尔排序的性能在实践中是可以接受的，由于编程简单，适度数量的输入数据经常选用。&lt;/p>
&lt;h2 id="堆排序-heap-sort">堆排序 (heap sort)&lt;/h2>
&lt;p>如第六章所说，优先队列可以用O(NlogN)时间进行排序，基于该思想的算法称为堆排序&lt;/p>
&lt;p>由数组建立N个元素的二叉堆花费O(N)时间，每次deleteMin花费O(logN)，N次总共花费O(NlogN)&lt;br>
使用了附加数组，存储需求增加了一倍&lt;/p>
&lt;p>避免使用附加数组的方法：每次deleteMin之后把min放到刚刚空出来的位置上，N次deleteMin之后，数组将是递减顺序，因此可以构建max堆&lt;/p>
&lt;ol>
&lt;li>以O(N)建立max堆&lt;/li>
&lt;li>交换最后一个和第一个元素，堆大小减1并下滤，相当于执行deleteMax&lt;/li>
&lt;li>循环执行步骤2，N-1次&lt;/li>
&lt;/ol>
&lt;h3 id="分析-2">分析&lt;/h3>
&lt;p>在最坏情形下堆排序最多使用2NlogN-O(N)次比较&lt;br>
堆排序非常稳定：它平均使用的比较只比最坏情形界指出的略少&lt;/p>
&lt;p>定理1 对N个互异项的随机排列进行堆排序，所用的比较平均次数为2NlogN-O(NloglogN)&lt;/p>
&lt;p>可以证明，堆排序总是至少使用NlogN-O(N)次比较，而且存在达到这个界的数据。似乎平均情形也应该是2NlogN-O(N)次比较（而不是定理1中的第二项），但目前无法证明&lt;/p>
&lt;h2 id="归并排序-merge-sort">归并排序 (merge sort)&lt;/h2>
&lt;p>以最坏情形O(NlogN)时间运行，所使用的比较次数几乎是最优的，它是递归算法的一个很好的实例&lt;/p>
&lt;p>算法的基本操作是合并两个已排序的表，取两个输入A、B，一个输出C，每次将A、B中的小者放入C，相关的位置推进，这显然是线性的&lt;/p>
&lt;h3 id="算法">算法&lt;/h3>
&lt;p>基准情形：N=1时，结果是显然的&lt;br>
否则，递归地将前半部分和后半部分各自归并排序，再将两部分合并&lt;/p>
&lt;p>该算法是经典的&lt;strong>分治&lt;/strong>策略，它将问题&lt;strong>分&lt;/strong>(divide)成一些小问题然后递归求解，而&lt;strong>治&lt;/strong>(conquering)的阶段则是将分的阶段解得的各答案合并在一起&lt;/p>
&lt;h3 id="分析-3">分析&lt;/h3>
&lt;p>分析递归例程技巧的经典实例：必须给运行时间写出一个递推关系。&lt;br>
假设N是2的幂，从而总可以将它分裂成相等的两部分。对于N=1，所用时间是常数，将其记为1。则有&lt;br>
T(1) = 1&lt;br>
T(N) = 2T(N/2) + N&lt;br>
求解得 T(N) = NlogN + N = O(NlogN)&lt;/p>
&lt;p>利弊：在java中比较耗时多于移动，因此在java中归并排序是一般目的排序的最佳选择；但在C++中，比较耗时少而复制对象代价很大，因此实践中不常用&lt;/p>
&lt;h2 id="快速排序-quick-sort">快速排序 (quick sort)&lt;/h2>
&lt;p>快排是实践中最快的已知排序算法，平均运行时间是O(NlogN)，最坏情形是O(N^2)，但稍作努力就可避免。&lt;br>
通过将堆排序与快速排序结合，可以在堆排序O(NlogN)最坏运行时间下，得到几乎所有输入的最快运行时间。&lt;/p>
&lt;p>快排也是分治的递归算法，排序数组S步骤如下：&lt;/p>
&lt;ol>
&lt;li>若S中元素数是0或1，则返回&lt;/li>
&lt;li>取S中任一元素v，称之为&lt;strong>枢纽元&lt;/strong>(pivot)&lt;/li>
&lt;li>将S-{v}（S中其余元素）&lt;strong>划分&lt;/strong>成两个不相交的集合：S1={x∈S-{v}|x≤v}和S2={x∈S-{v}|x≥v}&lt;/li>
&lt;li>返回{quickSort(S1),后跟v,继而quickSort(S2)}&lt;/li>
&lt;/ol>
&lt;p>第三步中划分的标准不是唯一的，因此这就成了设计决策。一部分好的实现方法是将这种情形尽可能有效地处理。直观地看，我们希望枢纽元能将元素对半分，一半在S1，另一半在S2。&lt;/p>
&lt;h3 id="选取枢纽元">选取枢纽元&lt;/h3>
&lt;ol>
&lt;li>一种典型的错误是将第一个元素选作枢纽元。若输入随机，那么这是可以接受的，但实际情况有很多预排序的序列，这样的分割是劣质的。类似的还有选取前2个元素的大者，这是一样的，不要使用。&lt;/li>
&lt;li>一种安全的做法是随机选取枢纽元，但这取决于随机数生成器的质量，而且声称随机数的代价一般也是很昂贵的。&lt;/li>
&lt;li>三数中值分割法&lt;br>
一组N个数的中值是第上取整(N/2)个最大的数。枢纽元的最好选择是数组的中值，但算出中值代价太高。一般的做法是选取左端、右端和中心位置上的三个元素的中值作为枢纽元。显然该方法消除了预排序输入的不好情形，并且减少了约14%的比较次数。&lt;/li>
&lt;/ol>
&lt;h3 id="分割策略">分割策略&lt;/h3>
&lt;ol>
&lt;li>将枢纽元与最后的元素交换&lt;/li>
&lt;li>i从第一个元素开始，j从倒数第二个元素开始&lt;/li>
&lt;li>当i在j左边时，右移i，移过小于枢纽元的元素，j左移，移过大于枢纽元的元素，i,j都停止时交换两个元素，直到i,j交错&lt;/li>
&lt;li>将枢纽元与i所指向的元素交换&lt;/li>
&lt;/ol>
&lt;p>如何处理等于枢纽元的元素？&lt;br>
若等于，则停止移动&lt;/p>
&lt;h3 id="小数组">小数组&lt;/h3>
&lt;p>对于很小的数组（N≤20），快速排序不如插入排序，而且，因为快排是递归的，这样的情形经常发生。通常的解决办法是，对于小数组使用插入排序。一种好的截止范围(cutoff range)是N=10&lt;/p>
&lt;h3 id="分析-4">分析&lt;/h3>
&lt;p>最坏情形：O(N^2)
最佳情形：O(NlogN)
平均情形：O(NlogN)&lt;/p>
&lt;h2 id="快速选择-quick-select">快速选择 (quick select)&lt;/h2>
&lt;p>修改快速排序以解决选择问题，即找第k个最大（小）元。&lt;/p>
&lt;p>前3步和快速排序一样&lt;br>
第4步&lt;/p>
&lt;ul>
&lt;li>若k≤S1，那么k必然在S1中，返回quickSelect(S1, K)&lt;/li>
&lt;li>若k = 1 + |S1|，那么枢纽元就是第k个最小元&lt;/li>
&lt;li>否则，第k个最小元就在S2中，它是S2中的第（k-|S1|-1）个最小元，返回quickSelect(S2, k-|S1|-1)&lt;/li>
&lt;/ul>
&lt;h3 id="分析-5">分析&lt;/h3>
&lt;p>与快排相比，快速选择只进行了一次递归调用而不是两次&lt;/p>
&lt;p>最坏情形：O(N^2)，当S1和S2一个是空时
平均情形：O(N)&lt;/p></description></item><item><title>数据结构学习笔记（六）：优先队列（堆）</title><link>https://i-square.github.io/p/Data-structure-study-notes-6-priority-queue-heap/</link><pubDate>Thu, 20 Apr 2017 22:27:15 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-6-priority-queue-heap/</guid><description>&lt;p>本章讨论优先队列（priority queue），介绍优先队列在离散事件模拟中的应用&lt;br>
作者评价：这类数据结构属于计算机科学中最雅致的一种&lt;/p>
&lt;h2 id="内容">内容&lt;/h2>
&lt;ul>
&lt;li>优先队列ADT的高效实现&lt;/li>
&lt;li>优先队列的使用&lt;/li>
&lt;li>优先队列的高级实现&lt;/li>
&lt;/ul>
&lt;h2 id="二叉堆-binary-heap">二叉堆 (binary heap)&lt;/h2>
&lt;p>插入删除最坏O(logN)，实际上插入花费常数平均时间，若无删除干扰，该结构将以线性时间建立一个具有N项的优先队列。&lt;br>
与二叉查找树一样，堆具有两个性质，堆的操作必须满足所有性质才能终止。&lt;/p>
&lt;h3 id="结构性质">结构性质&lt;/h3>
&lt;p>堆是一棵&lt;strong>完全二叉树&lt;/strong>（三角形缺右下角），特例是满二叉树（三角形），最底层元素必须从左往右填入，如有空缺则不是完全二叉树&lt;br>
一棵高为h的完全二叉树有[2^h , 2^(h+1) - 1]个节点，这意味着完全二叉树的高是 下取整(logN)，显然它是O(logN)的&lt;br>
因为此规律，所以堆可以用数组表示而不用链表，对于数组中任一位置i上的元素，其左儿子在位置2i上，右儿子在左儿子后的(2i+1)上，它的父亲在位置 下取整(i/2) 上&lt;/p>
&lt;h3 id="堆序性质">堆序性质&lt;/h3>
&lt;p>在堆中，除根节点以外，每一个节点的值都大于（或等于）它的父节点的值&lt;br>
根据堆序性质，最小值总在根结点，因此可以以O(1)时间做findMin&lt;br>
相应地，通过改变堆序性质，也可以建立一个max堆，以O(1)时间做findMax&lt;/p>
&lt;h3 id="插入上滤策略">插入（上滤策略）&lt;/h3>
&lt;p>为了插入新元素X，在堆的下一个可用位置（为了满足结构性质）创建一个空穴，若X放入空穴仍满足堆序性质，则插入完成，否则交换空穴和其父节点，直到X被放入并满足堆序性质为止&lt;/p>
&lt;h3 id="删除下滤策略">删除（下滤策略）&lt;/h3>
&lt;p>找出最小元很容易，难的是删除它。&lt;br>
当删除一个最小元时，堆中最后一个元素X必须移动到该堆的某个地方。策略是在根节点建立一个空穴，然后将两个儿子中的较小者移入空穴，重复该步骤直到X可以被放入空穴中。代码中则是用X直接替换根结点的值，然后下滤。&lt;/p>
&lt;h3 id="注意">注意&lt;/h3>
&lt;p>在堆的实现中经常出现的错误是，当堆中存在偶数个元素时，将出现一个节点只有一个儿子的情况。因此我们必须以节点不总有两个儿子为前提，这需要额外的测试。&lt;/p>
&lt;h3 id="应用">应用&lt;/h3>
&lt;h4 id="选择问题">选择问题&lt;/h4>
&lt;p>输入N个元素及整数k，找出第k个最大的元素，极端情况是k=上取整(N/2)，此时实际上是找中位数，以下两个算法都能在找中位数的情况下以O(NlogN)时间运行&lt;/p>
&lt;ul>
&lt;li>A 将N个元素读入数组，对数组应用buildHeap，再执行k次deleteMin，最后根节点上的就是第k个最小值，构造一个最大堆就可以找到第k个最大值&lt;/li>
&lt;li>B 用buildHeap将前k个元素构造成一个最大堆，若下一个元素大于堆里的最小值，则删除最小值，插入新元素，最终的最小值就是所求的第k个最大值&lt;/li>
&lt;/ul>
&lt;h2 id="d堆">d堆&lt;/h2>
&lt;p>类似B树，深度变浅，每个节点有d个儿子&lt;/p>
&lt;h2 id="左式堆-leftist-heap">左式堆 (leftist heap)&lt;/h2>
&lt;p>左式堆也是二叉树，但它不是理想平衡的，事实上是趋于非常不平衡&lt;/p>
&lt;p>定义任一节点X的**零路径长(null path length)**npl(X)为从X到一个不具有两个儿子的节点的最短路径长&lt;br>
因此，具有0个或1个儿子的节点npl为0，而npl(NULL)=-1&lt;br>
注意，任一节点的npl比它儿子节点的npl的最小值多1&lt;/p>
&lt;h3 id="左式堆性质">左式堆性质&lt;/h3>
&lt;p>对于堆中的每一个节点X，左儿子的npl至少与右儿子的npl一样大&lt;br>
这个性质导致树向左增加深度，沿左式堆右侧的右路径是堆中最短的路径&lt;br>
定理：在右路径上有r个节点的左式堆必然至少有2^r -1个节点&lt;/p>
&lt;p>对左式堆的基本操作是合并。插入可以看成是合并一个单节点堆，删除即是删掉根结点，然后合并左右子树。&lt;/p>
&lt;h2 id="斜堆-skew-heap">斜堆 (skew heap)&lt;/h2>
&lt;p>斜堆是左式堆的自调节形式，具有堆序，但不存在结构限制。斜堆不需要存储npl，每次合并无条件交换左右儿子。&lt;/p>
&lt;h2 id="二项队列-binomial-queue">二项队列 (binomial queue)&lt;/h2>
&lt;p>以最坏O(logN)支持插入、合并、deleteMin，插入操作平均花费常数时间&lt;/p>
&lt;p>实质是由&lt;strong>二项树&lt;/strong>(binomial tree)构成的&lt;strong>森林&lt;/strong>(forest)。&lt;br>
每一个高度上最多存在一棵二项树。高度为k的二项树Bk是通过将一棵二项树B(k-1)附接到另一棵二项树B(k-1)的根上构成的。高度为k的二项树有2^k个节点，在深度d处的节点数是二项系数C(d,k)&lt;/p>
&lt;p>如果把堆序性质施加到二项树上并允许任意高度上最多一棵二项树，则可以用二项树的集合唯一地表示任意大小的优先队列。如大小为13的优先队列可以用B3,B2,B0表示，可以写成1101，同时也是13的二进制形式。&lt;/p>
&lt;h3 id="操作">操作&lt;/h3>
&lt;p>基本操作仍然是合并，思想是从小到大合并相同高度的二项树&lt;br>
插入是特殊情况下的合并&lt;br>
deleteMin将原二项队列一分为二，再合并&lt;/p>
&lt;p>编程需要注意&lt;strong>进位&lt;/strong>的实现&lt;/p></description></item><item><title>数据结构学习笔记（五）：散列</title><link>https://i-square.github.io/p/Data-structure-study-notes-5-hash/</link><pubDate>Fri, 07 Apr 2017 22:48:51 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-5-hash/</guid><description>&lt;p>散列表（hash table）的实现通常称为散列（hashing），指用于以O(1)时间执行插入、删除和查找的技术，但不支持需要排序信息的树操作，比如findMin、findMax以及在线性时间内按顺序打印整个表都不支持&lt;/p>
&lt;h2 id="内容">内容&lt;/h2>
&lt;p>中心数据结构是&lt;strong>散列表&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>实现散列表的几种方法&lt;/li>
&lt;li>分析比较几种方法&lt;/li>
&lt;li>介绍散列的多种应用&lt;/li>
&lt;li>比较散列表与二叉查找树&lt;/li>
&lt;/ul>
&lt;h2 id="散列函数">散列函数&lt;/h2>
&lt;p>基本思想：将每个键(Key)映射到从[0, TableSize)这个范围中的某个数，并且将其放到适当的单元中，这个映射就称为&lt;strong>散列函数&lt;/strong>。&lt;br>
问题：选择一个函数，决定当两个键散列到同一个值的时候（称为**冲突(collision)**应该做什么以及如何确定散列表的大小。&lt;br>
&lt;em>注：一般使表的大小为素数，有助于避免部分冲突问题&lt;/em>&lt;/p>
&lt;h3 id="装填因子load-factor">装填因子(load factor)&lt;/h3>
&lt;p>定义散列表的装填因子 λ 为散列表中的元素个数与散列表大小的比值。&lt;/p>
&lt;h2 id="分离链接法">分离链接法&lt;/h2>
&lt;p>将散列到同一个值的所有元素保留到一个链表中。&lt;br>
一般法则：使 λ ≈ 1，控制链表的长度，若 λ &amp;gt; 1 则通过再散列扩充&lt;/p>
&lt;h2 id="开放定址法">开放定址法&lt;/h2>
&lt;p>不用链表存储，实现分配较大空间，称为&lt;strong>探测散列表&lt;/strong>&lt;br>
hi(x) = (hash(x) + f(i)) mod TableSize, f(0) = 0.&lt;br>
一般 λ &amp;gt; 0.5 就要再散列&lt;/p>
&lt;ul>
&lt;li>线性探测 f(i) = i&lt;/li>
&lt;li>平方探测 f(i) = i^2&lt;/li>
&lt;li>双散列 f(i) = i * hash2(x), hash2(x) = R - (x mod R) 这样的函数会起作用，其中R为小于TableSize的素数&lt;/li>
&lt;/ul>
&lt;h2 id="再散列rehash">再散列(rehash)&lt;/h2>
&lt;ol>
&lt;li>只要表到一半就再散列&lt;/li>
&lt;li>只有插入失败时才再散列&lt;/li>
&lt;li>途中策略：当表到达某一个装填因子时进行再散列（最优）&lt;/li>
&lt;/ol></description></item><item><title>数据结构学习笔记（四）：树</title><link>https://i-square.github.io/p/Data-structure-study-notes-4-tree/</link><pubDate>Fri, 31 Mar 2017 22:05:29 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-4-tree/</guid><description>&lt;h2 id="内容">内容&lt;/h2>
&lt;ul>
&lt;li>了解树是如何用于实现文件系统的&lt;/li>
&lt;li>了解树如何用来计算算术表达式的值&lt;/li>
&lt;li>了解如何用树实现O(logN)时间进行搜素&lt;/li>
&lt;li>讨论并使用set和map&lt;/li>
&lt;/ul>
&lt;h2 id="二叉树的遍历">二叉树的遍历&lt;/h2>
&lt;ul>
&lt;li>前序：先处理自己后处理左右儿子&lt;/li>
&lt;li>中序：先处理左儿子再处理自己再处理右儿子&lt;/li>
&lt;li>后序：先处理左右儿子再处理自己&lt;/li>
&lt;/ul>
&lt;h2 id="二叉查找树平均深度ologn">二叉查找树（平均深度O(logN)）&lt;/h2>
&lt;p>性质：对于树中的每个节点X，左子树中所有项的值小于X中的项，右子树中所有项的值大于X中的项&lt;br>
缺点：不能动态调整，若输入为已排序序列则构造出最坏情况下的斜树&lt;/p>
&lt;h2 id="avl树">AVL树&lt;/h2>
&lt;ul>
&lt;li>带有&lt;strong>平衡条件&lt;/strong>的二叉查找树&lt;/li>
&lt;li>一棵AVL树是每个节点的左子树和右子树的高度最多相差1的二叉查找树（空树高度定义为-1）&lt;/li>
&lt;li>插入新节点可能破坏AVL树的平衡，需要通过&lt;strong>旋转&lt;/strong>解决&lt;/li>
&lt;/ul>
&lt;p>把需要平衡的节点叫α&lt;/p>
&lt;ol>
&lt;li>对α的左儿子的左子树进行一次插入&lt;/li>
&lt;li>对α的左儿子的右子树进行一次插入&lt;/li>
&lt;li>对α的右儿子的左子树进行一次插入&lt;/li>
&lt;li>对α的右儿子的右子树进行一次插入&lt;/li>
&lt;/ol>
&lt;p>1和4（左左，右右）发生在外边，进行一次&lt;strong>单旋转&lt;/strong>即可，2和3（左右，右左）则发生在内部，需要通过&lt;strong>双旋转&lt;/strong>调整&lt;/p>
&lt;h2 id="伸展树">伸展树&lt;/h2>
&lt;p>节点可以达到任意深度，每次访问某节点后把该节点调整为根节点，任意连续M次操作花费O(MlogN)时间&lt;/p>
&lt;h2 id="b树平衡m路树">B树（平衡M路树）&lt;/h2>
&lt;p>M=3时：2-3树，实现平衡查找树的另一种方法&lt;/p>
&lt;h2 id="注意">注意&lt;/h2>
&lt;p>通过插入元素构造查找树，然后执行中序遍历，可以得到排序后的元素。&lt;br>
这是一种O(NlogN)的排序算法&lt;/p></description></item><item><title>数据结构学习笔记（三）：表、栈和队列</title><link>https://i-square.github.io/p/Data-structure-study-notes-3-tables-stacks-and-queues/</link><pubDate>Mon, 27 Mar 2017 15:27:43 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-3-tables-stacks-and-queues/</guid><description>&lt;h2 id="内容">内容&lt;/h2>
&lt;ul>
&lt;li>介绍三种基本的数据结构&lt;/li>
&lt;li>介绍抽象数据类型(ADT, abstract data type)的概念&lt;/li>
&lt;li>介绍栈ADT及其在实现递归方面的应用&lt;/li>
&lt;li>介绍队列ADT及其在操作系统和算法设计中的应用&lt;/li>
&lt;li>给出vector和list的重要子集的实现&lt;/li>
&lt;/ul>
&lt;h2 id="栈">栈&lt;/h2>
&lt;h3 id="实现">实现&lt;/h3>
&lt;p>栈是一个表，因此任何实现表的方法都能实现栈。&lt;/p>
&lt;h3 id="应用">应用&lt;/h3>
&lt;ol>
&lt;li>符号平衡&lt;/li>
&lt;li>后缀（逆波兰）表达式计算&lt;/li>
&lt;li>中缀到后缀的转换&lt;/li>
&lt;li>函数调用&lt;br>
（代码实现了一个简单的计算器，应保证输入合法）&lt;/li>
&lt;/ol>
&lt;h2 id="总结">总结&lt;/h2>
&lt;h3 id="快慢指针">快慢指针&lt;/h3>
&lt;p>ex 3.34 提示：判断一个链表是否有环，只使用O(1)的额外空间，使用两个迭代器p,q p每次递增1，q每次递增2，若q到了末尾则没环，否则pq必定在环中间相遇&lt;/p>
&lt;p>也可用于快速找出单链表的中间节点&lt;/p></description></item><item><title>数据结构学习笔记（二）：算法分析</title><link>https://i-square.github.io/p/Data-structure-study-notes-2-algorithm-analysis/</link><pubDate>Sat, 25 Mar 2017 22:48:53 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-2-algorithm-analysis/</guid><description>&lt;h2 id="内容">内容&lt;/h2>
&lt;ul>
&lt;li>主要内容是复杂度分析&lt;/li>
&lt;li>大O标记&lt;/li>
&lt;li>计算大O时的一般法则
&lt;ul>
&lt;li>对数规律的一般法则&lt;br>
如果一个算法用常数时间（O(1)）将问题的大小削减为其一部分（通常是1/2），那么该算法就是O(logN)的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="例子">例子&lt;/h2>
&lt;ol>
&lt;li>二分搜索提供了O(logN)的查找算法&lt;/li>
&lt;li>最大公因数的欧几里得算法也是O(logN)的&lt;/li>
&lt;li>幂运算的递归算法&lt;/li>
&lt;/ol></description></item><item><title>数据结构学习笔记（一）：引论</title><link>https://i-square.github.io/p/Data-structure-study-notes-1-introduction/</link><pubDate>Fri, 24 Mar 2017 14:22:23 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-1-introduction/</guid><description>&lt;h2 id="内容">内容&lt;/h2>
&lt;ul>
&lt;li>介绍基本数学知识&lt;/li>
&lt;li>简要复习递归&lt;/li>
&lt;li>介绍用到的C++知识&lt;/li>
&lt;/ul>
&lt;h2 id="递归的四条基本法则">递归的四条基本法则&lt;/h2>
&lt;ol>
&lt;li>基准情形。必须总有某些基准情形不用递归就能求解。&lt;/li>
&lt;li>不断推进。对于那些需要递归求解的情形，递归调用必须总能够朝着基准情形的方向推进。&lt;/li>
&lt;li>设计法则。假设所有的递归调用都能运行。&lt;/li>
&lt;li>合成效益法则。在求解一个问题的同一实例时，切勿在不同的递归调用中做重复性的工作。&lt;/li>
&lt;/ol></description></item><item><title>数据结构学习笔记（零）：开始</title><link>https://i-square.github.io/p/Data-structure-study-notes-0-start/</link><pubDate>Thu, 23 Mar 2017 11:23:54 +0000</pubDate><guid>https://i-square.github.io/p/Data-structure-study-notes-0-start/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>为了准备今年后半年到来的秋招，我决定开始学习数据结构，为后面的学习打基础，采用的教材是weiss的《数据结构与算法分析C++描述》，计划实现书上上的示例代码以及力所能及的课后习题。&lt;/p>
&lt;p>在Github上同步源码，项目地址：&lt;a class="link" href="https://github.com/i-square/Data-Structure" target="_blank" rel="noopener"
>https://github.com/i-square/Data-Structure&lt;/a>&lt;/p>
&lt;h2 id="学习环境">学习环境&lt;/h2>
&lt;ul>
&lt;li>Windows 10 &amp;amp; 8.1&lt;/li>
&lt;li>Visual Studio 2015 with update 3&lt;/li>
&lt;li>C++ (部分C++11语法)&lt;/li>
&lt;/ul></description></item></channel></rss>